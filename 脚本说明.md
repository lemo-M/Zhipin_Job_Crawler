在插入数据库的时候，如果插入失败，会有临时文件保存已经爬取下来的数据，通过检查文件的创建时间（小于6h），来判断是否需要重新爬取数据

**脚本说明：**

使用前修改数据库配置
DB_CONFIG = {

    'host': 'localhost',
    'user': 'root',
    'password': 'your_password',
    'database': 'ai',
    'charset': 'utf8mb4'

}

1.  **开始**: 脚本启动。
2.  **检查 'cryptography' 是否安装?**: 一个预检查，如果关键依赖缺失则提示，主要解决MySQL5.x和MySQL8.x之间的密码策略不同的问题。
3.  **脚本主逻辑 / 加载配置**: 初始化基本设置。
4.  **初始化 BossZhipinCrawler**: 创建爬虫实例。
5.  **检查是否存在 TEMP\_DATA\_FILE?**: 判断是否有上次运行留下的临时数据。
    *   **是**: 继续检查文件是否“新鲜”。
    *   **否**: 直接进入爬虫数据路径 (`FetchDataPath`)。
6.  **TEMP\_DATA\_FILE 是否在 MAX\_DATA\_AGE\_HOURS 内?**:
    *   **是**: 尝试从文件加载数据。
    *   **否**: 文件过旧，进入爬虫数据路径 (`FetchDataPath`)。
7.  **尝试从 TEMP\_DATA\_FILE 加载数据**:
    *   **加载成功**: 数据被赋值给 `all_cities_district_data`，跳过爬虫步骤。
    *   **加载失败**: 文件可能损坏或不存在，进入爬虫数据路径 (`FetchDataPath`)。
8.  **FetchDataPath (执行爬虫获取数据)**:
    *   **步骤 1: fetch\_city\_list()**: 获取城市列表。失败则记录并终止。
    *   **步骤 2: batch\_fetch\_district\_data()**: 批量获取商圈数据。失败则记录并终止。
    *   **保存 all\_cities\_district\_data 到 TEMP\_DATA\_FILE**: 爬虫成功后，将结果持久化到临时文件。
    *   `all_cities_district_data` 被赋予新爬取的数据。
9.  **all\_cities\_district\_data 是否有效?**: 检查是否有数据可供后续处理（无论是从文件加载还是新爬取的）。
    *   **是**: 继续后续的数据处理和入库步骤。
    *   **否**: 没有数据，记录错误并终止。
10. **步骤 3: \_save\_json\_data\_to\_file\_and\_db**: 将 `all_cities_district_data` 保存到永久的JSON文件，并存入数据库的 `city_json_storage` 表。如果此步骤失败（例如数据库连接问题），记录错误并终止。
11. **步骤 4: process\_and\_insert\_data\_into\_region**: 解析 `all_cities_district_data` 并将层级数据插入到 `region` 表。如果此步骤失败，记录错误并终止。
12. **打印成功信息**: 所有步骤成功完成。
13. **结束**: 脚本执行完毕。

各个分支的错误处理（如 `I_Error`, `J_Error`, `M_Error`, `N_Error`, `L_Error`）都会导致记录日志并可能终止脚本，防止错误继续传播。
